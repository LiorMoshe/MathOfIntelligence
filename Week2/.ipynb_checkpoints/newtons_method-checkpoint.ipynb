{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of the coding challenge of week2 of the Math of Intelligence course.\n",
    "In this challenge we will implement newston's method that is used for root finding and second order optimization using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy import *\n",
    "from sympy.parsing import sympy_parser as spp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "init_printing(use_unicode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will look at newton's method to find roots of a polynomial (also called newton-raphson method).\n",
    "\n",
    "#### What is a root?\n",
    "For a polynomial f(x) a root x0 of this polynomial upholds:\\begin{equation*}\n",
    "f(x_0) = 0\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The idea\n",
    "First, let's look at the problem of finding roots of a function f(x) in 2D.\n",
    "By using newton's method we will start with a random guess $x_0$ and update our guess using the following update rule:\n",
    "$x_{n+1} = x_n - f(x_n) / f'(x_n)$\n",
    "We will use this update rule iteratively until a good approximation is reached.\n",
    "Let's understands the meaning of this update rule, $f'(x_n)$ stands for the **derivative** of our polynomial at the point of our current root guess $x_n$. The derivative of our polynomial f by x indicates the **rate of change** of f relatively to x if we will change x by a small margin.If it's positive it means the value f(x) will increase and if it's negative it means it will decrease.\n",
    "So the expression $f(x_n)/f'(x_n)$ will **always** go at the direction of the root.Why? Because if our polynomial increases at $x_n$ our update rule will reduce the absolute value of our root guess (whether it was positive or negative,same for $f(x_n)$),same for the case that the polynomial decreases at $x_n$.\n",
    "\n",
    "When we will stop using the update rule and get our final guess?\n",
    "We will use some boundary value $epsilon$ such that when $abs(x_n) < epsilon$ we will stop iterating.\n",
    "\n",
    "Cool graph that shows our update rule:\n",
    "![title](root_img.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get to the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dx(f,x):\n",
    "    return abs(f(x))\n",
    "\n",
    "def newton_root_find(f,df,x0,e):\n",
    "    '''\n",
    "    Perform the newton-raphson method to find a root of a given function f.\n",
    "    Parameters:\n",
    "        -f: The function that we will find it's root.\n",
    "        -df: The function's derivative.\n",
    "        -x0: Our starting guess.\n",
    "        -e : Our boundary epsilon,when abs(x0) will be lower than it we\n",
    "             will stop.\n",
    "    '''\n",
    "    delta = dx(f,x0)\n",
    "    while delta > e:\n",
    "        x0 = x0 - f(x0)/df(x0)\n",
    "        delta = dx(f,x0)\n",
    "    print(\"Final root: \" + str(x0))\n",
    "    print(\"Value at root: \" + str(f(x0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our implementation using some sample function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final root: 0.875\n",
      "Value at root: 0.044921875\n"
     ]
    }
   ],
   "source": [
    "def plot_formula(formula,x_range):\n",
    "    '''\n",
    "    Plot a 2d graph using a given formula.\n",
    "    '''\n",
    "    x = np.array(x_range)\n",
    "    y = formula(x)\n",
    "    plt.plot(x,y)\n",
    "    plt.show()\n",
    "\n",
    "def example_func(x):\n",
    "    return x**3 + 5*x -5\n",
    "\n",
    "plot_formula(example_func,np.arange(-50,50,.5))\n",
    "derivative = lambda x: 3 * x ** 2 + 5\n",
    "newton_root_find(example_func,derivative,1,0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton optimization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from gradient descent which we looked at the previous lesson,newton's optimization method looks at the **second derivative** of our loss function in order to optimize our loss (in case of multidimensional functions we will refer to the **Hassian Matrix**).\n",
    "In the optimization case we will try to find the value $x_n$ such that our derivative will be equal to zero: $f'(x_n) = 0$. So it's pretty trivial (basic calculus) that when our derivative will get closer to zero we will reach **local minima**.\n",
    "\n",
    "There are two cases for implementing newton's optimization method: the 1D case and the multi-dimensional case.\n",
    "We will implement the method for both cases.\n",
    "\n",
    "First let's implement the optimization process for the first order optimization case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The range of our plot\n",
    "plot_from,plot_to,plot_step = -7.0,7.0,0.1\n",
    "\n",
    "#The range of precision we will target.\n",
    "target_precision = 0.3\n",
    "\n",
    "#Create a matrix of the symbols that we will use.\n",
    "m = Matrix(symbols('x1 x2'))\n",
    "\n",
    "#Obj will save the expression that we will optimize.\n",
    "obj = spp.parse_expr('x1**2 -2 * x1 * x2 + 4 * x2**2')\n",
    "\n",
    "def dfdx(x,gradient):\n",
    "    '''\n",
    "    Compute value of the gradient at a given point x.\n",
    "    Parameters:\n",
    "        - x : The value we will compute the gradient at.\n",
    "        - gradient: The gradient (using sympy expression)\n",
    "    Returns:\n",
    "        - value of the gradient at x.\n",
    "    '''\n",
    "    return [gradient[i].subs(m[0],x[0]).subs(m[1],x[1]) for i in range(len(gradient))]\n",
    "\n",
    "def steepest_descent(obj,x_start,x_res,lr = 0.0002):\n",
    "    '''\n",
    "    This is a first order optimization algorithm.\n",
    "    Parameters:\n",
    "        -lr: Stands for the learning rate that we will use in our\n",
    "             update rule.\n",
    "        -obj: Sympy expression that we will optimize.\n",
    "        -x_start: Starting point for our optimization.\n",
    "        -x_res: Ending point for our optimization.\n",
    "    Returns: void\n",
    "    \n",
    "    '''\n",
    "    #First compute the gradient,derivative by each variable.\n",
    "    gradient = [diff(obj,i) for i in m]\n",
    "    #Initialize our current value (default will be 0).\n",
    "    x_vals = [[0.0,0.0]]\n",
    "    #Save current number of iteration.\n",
    "    num_iter = 0\n",
    "    #Loop until the distance between the previous iteration value\n",
    "    # to the current value is smaller than our target distance.\n",
    "    while (np.linalg.norm(np.array(x_vals[-1]) - x_res).astype(int) \n",
    "           > target_precision):\n",
    "        #Compute gradients at current point\n",
    "        grads = dfdx(x_vals[num_iter],gradient)\n",
    "        \n",
    "        #Update current value.\n",
    "        x_vals.append(x_vals[num_iter] - alpha * grads)\n",
    "        num_iter += 1\n",
    "        #Stop after a thousand iterations\n",
    "        if (num_iter > 1000):\n",
    "            break\n",
    "    print(\"Steepest descent final result distance: \",\n",
    "          np.linalg.norm(np.array(x_vals[-1]) - x_res).astype(int) )\n",
    "    #Plot our results.\n",
    "    x_vals = np.array(x_vals)\n",
    "    plt.plot(x_vals[:,0],x_vals[:,1],'g-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the same optimization process for second order optimization using newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_optimization(obj,x_start,x_res):\n",
    "    '''\n",
    "    Newton's method for second order optimization.\n",
    "    Parameters:\n",
    "        -obj: Sympy expression that we will optimize.\n",
    "        -x_start: Starting point for our optimization.\n",
    "        -x_res: Ending point for our optimization.\n",
    "    '''\n",
    "    #Compute the gradient.\n",
    "    gradient = [diff(obj,i) for i in m]\n",
    "    \n",
    "    #Hessian matrix.\n",
    "    H = Matrix([[diff(partial_deriv,i) for i in m]\n",
    "                for partial_deriv in gradient])\n",
    "    H_inv = H.inv()\n",
    "    \n",
    "    x_vals = [[0,0]]\n",
    "    x_vals[0] = x_start\n",
    "    #Compute the amount we will change our current value xn by:delta\n",
    "    delta = -H_inv * gradient\n",
    "    \n",
    "    num_iter = 0\n",
    "    #Loop until the distance between our current point to the final one\n",
    "    #is smaller that the target precision.\n",
    "    while(np.linalg.norm(\n",
    "        (np.array(x_vals[-1]) - x_res).astype(int)) > target_precision):\n",
    "        #grads = Matrix([dfdx()])\n",
    "        #Evaluate delta's value for x_n.\n",
    "        delta_xn = delta.subs(m[0],x_vals[num_iter][0]).\n",
    "                    subs(m[1],x_vals[num_iter][1])\n",
    "        x_vals.append(x_vals[num_iter] + delta_xn)\n",
    "    \n",
    "    print(\"Final result newton's method: \",np.linalg.norm(\n",
    "        (np.array(x_vals[-1]) - x_res).astype(int)) > target_precision)\n",
    "    \n",
    "    #Plot our final results.\n",
    "    x_vals = np.array(x_vals)\n",
    "    plt.plot(x_vals[0,:],x_vals[1,:],'k-o')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

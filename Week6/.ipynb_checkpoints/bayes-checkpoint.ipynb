{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a classification algorithm that relies on the famous Bayes theorem from the field of probability theory. <br>\n",
    "This theorem states that for two events $A$ and $B$ we can compute the probability of A happening given that B already happened,also denoted as $\\Pr(A | B)$ using the following equation:\n",
    "$$\\Pr(A | B) = \\frac{\\Pr(B | A) * \\Pr(A)}{\\Pr(B)}$$\n",
    "\n",
    "**Some terms**:<br>\n",
    "$\\Pr(A | B)$ = termed the **posterior probability**. The probability of A happening after B already happened.<br>\n",
    "$\\Pr(A)$ = termed the **prior probability**, the original probability of A happening prior to the stage that B happened.<br>\n",
    "$\\Pr(B)$ = termed the **evidence probability**, this is the probability of B happening (called evidence because this event already happened).\n",
    "$\\Pr(B | A)$ = termed the **likelihood probability**, what is the likelihood that given A we have B (in classification terms you can think of it as the probability of having some features knowing the class).\n",
    "\n",
    "### How this works?\n",
    "So how can we get a full classification algorithm from using Bayes theorem. Let's first think about the properties of a classification task. In this task we get several examples where each example contain several features (also called sometimes **predictors**) and our job is to **classify** each example based on the given features. If we shift that classification task to the world of probability theory we can look at a classification as computation of the probability of certain events happening.<br>\n",
    "Say we have $k$ classes $c_1,c_2,...,c_k$ and n features $x_1,x_2,...,x_n$, we can think on each class as a **probablistic event** and each feature as a [random variable](https://en.wikipedia.org/wiki/Random_variable).<br>\n",
    "Now let's say we are given an example where all our features are equal to 1 $x_1=1,x_2=1,...,x_n=1$ we can compute the probability that this example classifies to be the class $c_1$ as:\n",
    "$$\\Pr(c_1 | x_1=1,x_2=1,...,x_n=1) = \\frac{\\Pr(x_1=1,x_2=1,...,x_n=1 | c_1) * \\Pr(c_1)}{\\Pr(x_1=1,x_2=1,...,x_n=1)}$$ <br>\n",
    "\n",
    "Great! Now all we have left is understand how we can compute these probabilities and we can set up our classifier.<br>\n",
    "\n",
    "But wait, if we have n features and k classes and let's mark the number of values each feature can take as $m$ we have to compute $k * m ^ n$ which can sum up to **billions** of computations. That's why inorder to use Naive Bayes we have to limit our dataset to one imporant condition:<br>\n",
    "All of our features need to be [**completely independent**](http://www.statisticshowto.com/independent-random-variables/) of each other,meaning:\n",
    "$$\\Pr(x_1=1,x_2=1,...,x_n=1) = \\Pr(x_1=1) * \\Pr(x_2=1) * ... * \\Pr(x_n=1)$$\n",
    "\n",
    "As a result our computation using bayes theorem is much easier:\n",
    "$$\\Pr(c_1 | x_1=1,x_2=1,...,x_n=1) = \\frac{\\Pr(x_1=1| c_1) * \\Pr(x_2=1| c_1) * ... * \\Pr(x_n=1| c_1) * \\Pr(c_1)}{\\Pr(x_1=1,x_2=1,...,x_n=1)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test the naive bayes algorithm on the task of sentiment analysis of the given dataset.\n",
    "Our dataset contains reviews from amazon,imdb and yelp and our task is to classify each review as positive (labeled 1) or negative (labeled 0), we will use the naive bayes algorithm meaning that we will rely on the fact that each feature (in this case the words in the review) is completely independent of another feature.\n",
    "We will simply create a frequency table for each word in our training data and compute the probability of having each label over each review in the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_dataframe_from_txt(filePath):\n",
    "    df = pd.DataFrame(columns=('Text','Label'))\n",
    "    for index,line in enumerate(open(filePath,\"r\").readlines()):\n",
    "        line = line.replace('\\n','')\n",
    "        df.loc[index] = line.split('\\t')\n",
    "    return df\n",
    "df_amz = produce_dataframe_from_txt(\"data/amazon.txt\")\n",
    "df_imdb = produce_dataframe_from_txt(\"data/imdb.txt\")\n",
    "df_yelp = produce_dataframe_from_txt(\"data/yelp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have to jiggle the plug to get it to line up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If you have several dozen or several hundred c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>If you are Razr owner...you must have this!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Needless to say, I wasted my money.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What a waste of money and time!.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>And the sound quality is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>He was very impressed when going from the orig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>If the two were seperated by a mere 5+ ft I st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Very good quality though</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The design is very odd, as the ear \"clip\" is n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Highly recommend for any one who has a blue to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I advise EVERYONE DO NOT BE FOOLED!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>So Far So Good!.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Works great!.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It clicks into place in a way that makes you w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I went on Motorola's website and followed all ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I bought this to use with my Kindle Fire and a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The commercials are the most misleading.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I have yet to run this new battery below two b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I bought it for my mother and she had a proble...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Great Pocket PC / phone combination.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I've owned this phone for 7 months now and can...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I didn't think that the instructions provided ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>People couldnt hear me talk and I had to pull ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Doesn't hold charge.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>I immediately said I wanted to talk to the man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>The ambiance isn't much better.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>Unfortunately, it only set us up for disapppoi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>The food wasn't good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>Your servers suck, wait, correction, our serve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>What happened next was pretty....off putting.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>too bad cause I know it's family owned, I real...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Overpriced for what you are getting.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>I vomited in the bathroom mid lunch.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>I kept looking at the time and it had soon bec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>I have been to very few places to eat that und...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>We started with the tuna sashimi which was bro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>Food was below average.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>It sure does beat the nachos at the movies but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>All in all, Ha Long Bay was a bit of a flop.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>The problem I have is that they charge $11.99 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>Shrimp- When I unwrapped it (I live only 1/2 a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>It lacked flavor, seemed undercooked, and dry.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>It really is impressive that the place hasn't ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>I would avoid this place if you are staying in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>The refried beans that came with my meal were ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>Spend your money and time some place else.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>A lady at the table next to us found a live gr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>the presentation of the food was awful.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>I can't tell you how disappointed I was.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text Label\n",
       "0    So there is no way for me to plug it in here i...     0\n",
       "1                          Good case, Excellent value.     1\n",
       "2                               Great for the jawbone.     1\n",
       "3    Tied to charger for conversations lasting more...     0\n",
       "4                                    The mic is great.     1\n",
       "5    I have to jiggle the plug to get it to line up...     0\n",
       "6    If you have several dozen or several hundred c...     0\n",
       "7          If you are Razr owner...you must have this!     1\n",
       "8                  Needless to say, I wasted my money.     0\n",
       "9                     What a waste of money and time!.     0\n",
       "10                     And the sound quality is great.     1\n",
       "11   He was very impressed when going from the orig...     1\n",
       "12   If the two were seperated by a mere 5+ ft I st...     0\n",
       "13                            Very good quality though     1\n",
       "14   The design is very odd, as the ear \"clip\" is n...     0\n",
       "15   Highly recommend for any one who has a blue to...     1\n",
       "16                 I advise EVERYONE DO NOT BE FOOLED!     0\n",
       "17                                    So Far So Good!.     1\n",
       "18                                       Works great!.     1\n",
       "19   It clicks into place in a way that makes you w...     0\n",
       "20   I went on Motorola's website and followed all ...     0\n",
       "21   I bought this to use with my Kindle Fire and a...     1\n",
       "22            The commercials are the most misleading.     0\n",
       "23   I have yet to run this new battery below two b...     1\n",
       "24   I bought it for my mother and she had a proble...     0\n",
       "25                Great Pocket PC / phone combination.     1\n",
       "26   I've owned this phone for 7 months now and can...     1\n",
       "27   I didn't think that the instructions provided ...     0\n",
       "28   People couldnt hear me talk and I had to pull ...     0\n",
       "29                                Doesn't hold charge.     0\n",
       "..                                                 ...   ...\n",
       "970  I immediately said I wanted to talk to the man...     0\n",
       "971                    The ambiance isn't much better.     0\n",
       "972  Unfortunately, it only set us up for disapppoi...     0\n",
       "973                              The food wasn't good.     0\n",
       "974  Your servers suck, wait, correction, our serve...     0\n",
       "975      What happened next was pretty....off putting.     0\n",
       "976  too bad cause I know it's family owned, I real...     0\n",
       "977               Overpriced for what you are getting.     0\n",
       "978               I vomited in the bathroom mid lunch.     0\n",
       "979  I kept looking at the time and it had soon bec...     0\n",
       "980  I have been to very few places to eat that und...     0\n",
       "981  We started with the tuna sashimi which was bro...     0\n",
       "982                            Food was below average.     0\n",
       "983  It sure does beat the nachos at the movies but...     0\n",
       "984       All in all, Ha Long Bay was a bit of a flop.     0\n",
       "985  The problem I have is that they charge $11.99 ...     0\n",
       "986  Shrimp- When I unwrapped it (I live only 1/2 a...     0\n",
       "987     It lacked flavor, seemed undercooked, and dry.     0\n",
       "988  It really is impressive that the place hasn't ...     0\n",
       "989  I would avoid this place if you are staying in...     0\n",
       "990  The refried beans that came with my meal were ...     0\n",
       "991         Spend your money and time some place else.     0\n",
       "992  A lady at the table next to us found a live gr...     0\n",
       "993            the presentation of the food was awful.     0\n",
       "994           I can't tell you how disappointed I was.     0\n",
       "995  I think food should have flavor and texture an...     0\n",
       "996                           Appetite instantly gone.     0\n",
       "997  Overall I was not impressed and would not go b...     0\n",
       "998  The whole experience was underwhelming, and I ...     0\n",
       "999  Then, as if I hadn't wasted enough of my life ...     0\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_amz,df_imdb,df_yelp])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map each word to a tuple of negative and positive.\n",
    "frequency_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_sentences_from_dataframe(data):\n",
    "    return {tuple(data.iloc[row_num][0].split(' ')):int(data.iloc[row_num][1])\n",
    "            for row_num in range(data.shape[0])}\n",
    "\n",
    "def process_data(data):\n",
    "    '''\n",
    "    Take the data that we got and produce the following measures:\n",
    "    positive_rate: Ratio of positive reviews in our data.\n",
    "    negative_rate: Ratio of negative reviews in our data.\n",
    "    pos_total:Total positive reviews.\n",
    "    neg_total:Total negative reviews.\n",
    "    '''\n",
    "    positive = 0;negative = 0;pos_total = 0;neg_total = 0.0\n",
    "    pos_total_words = 0.0;neg_total_words = 0.0\n",
    "    \n",
    "    sentences = produce_sentences_from_dataframe(data)\n",
    "    for sentence,label in sentences.items():\n",
    "        if label == 1:\n",
    "            pos_total_words += len(sentence)\n",
    "            pos_total += 1\n",
    "        else:\n",
    "            neg_total_words += len(sentence)\n",
    "            neg_total += 1\n",
    "        process_sentence(sentence,label)\n",
    "        \n",
    "    positive_rate = pos_total / float(data.shape[0])\n",
    "    negative_rate = neg_total / float(data.shape[0])\n",
    "    return positive_rate,negative_rate,pos_total,neg_total,pos_total_words,neg_total_words\n",
    "    \n",
    "def process_sentence(sentence,label):\n",
    "    '''\n",
    "    Pass over each sentence and add 1 to the count of each word in\n",
    "    the table in the frequency table.\n",
    "    '''\n",
    "    for word in sentence:\n",
    "        if word in frequency_table:\n",
    "            frequency_table[word][label] += 1\n",
    "        else:\n",
    "            frequency_table[word] = [0,0]\n",
    "            frequency_table[word][label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_to_prob():\n",
    "    '''\n",
    "    Convert our frequency table to probability measures of each class (in our case it's binary).\n",
    "    Notice that we use a deep copy to copy our table and not change the value of the frequency table.\n",
    "    '''\n",
    "    probability_table = copy.deepcopy(frequency_table)\n",
    "    for key,val in probability_table.items():\n",
    "        probability_table[key][:] = [x / float(sum(probability_table[key])) for x in probability_table[key]]\n",
    "    return probability_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset randomly to training and test set.\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "train = df[mask]\n",
    "test = df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_table = {}\n",
    "pos,neg,pos_total,neg_total,pos_total_words,neg_total_words = process_data(train)\n",
    "probability_table = freq_to_prob()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = [neg_total_words,pos_total_words]\n",
    "totals = [neg_total,pos_total]\n",
    "def conditional_prob(word,label):\n",
    "    '''\n",
    "    Compute the probability of a word belonging to a document of class label.\n",
    "    This is the BAD way of computing probability without using laplace smoothing.\n",
    "    '''\n",
    "    if word in frequency_table:\n",
    "        return frequency_table[word][label] / total_words[label]\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "def classify(data,probs_calc = conditional_prob):\n",
    "    '''\n",
    "    Classify all our data using naive bayes theorem.\n",
    "    '''\n",
    "    probs = np.array([pos,neg])\n",
    "    for word in data:\n",
    "        probs *= np.array([probs_calc(word,0),probs_calc(word,1)])\n",
    "    return np.argmax(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6992\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(probs_calc = conditional_prob):\n",
    "    good = 0.0;bad = 0.0\n",
    "    my_test = test.copy()\n",
    "    for i in range(len(my_test)):\n",
    "        curr_data = my_test.iloc[i]\n",
    "        curr_data[0] = curr_data[0].split(' ')\n",
    "        prediction = classify(curr_data[0],probs_calc)\n",
    "        if prediction == int(curr_data[1]):\n",
    "            good += 1\n",
    "        else:\n",
    "            bad += 1\n",
    "    print(\"Accuracy: \" + str((good) / (good + bad)))\n",
    "\n",
    "test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14622.0, 14814.0]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks awesome,we got to 70% accuracy using only probabilities and bayes theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of Naive Bayes\n",
    "Pros:<br>\n",
    "1. Fast to compute.<br>\n",
    "2. Can be easily implemented.<br>\n",
    "3. Works well for very high dimensions.<br>\n",
    "\n",
    "Cons:<br>\n",
    "1. Relies on the assumption that the features are independent and will work very bad if this assumption is not met.<br>\n",
    "2. Needs large datasets to work well.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace Smoothing\n",
    "\n",
    "An important case that I didn't refer to is the case of an unknown token (meaning word) in the test data.\n",
    "Currently in our code we use the following equation to compute the probability of a token residing in a document,given that it's of class C:\n",
    "$$Pr(w|c) = \\frac{Count\\space of\\space w\\space in\\space documents\\space of\\space class\\space C}{Count\\space of\\space total\\space words\\space in\\space documents\\space of\\space class\\space C}$$\n",
    "\n",
    "If we use this equation for a word that we haven't seen in the training data we will get a probability of **0**!<br>\n",
    "In the code that I written before if a word is not in our training data I simply ignored it, **which is completely wrong and boneheaded thing to do**.\n",
    "\n",
    "This is a major problem because it zeroes out the probability of our whole document being of class C:<br>\n",
    "Consider the case of having a very positive review where the classifier assigns it a probability of 0.96 of being positive and 0.04 of being negative,if the reviewer decides to edit the review and add in a word that our naive bayes classifier didn't know before it will cause the probabilities of the review being positive and negative to **sink** to zero.<br>\n",
    "<br>\n",
    "We solve this problem by using a technique called **Laplace Smoothing**, the idea is that we will add a special UNK word (meaning unknown) that all of it's counts for each class C will be set to 0 and we will use the following equation to compute the probability of a word to be in a document given that it's class is C:\n",
    "$$Pr(w|c) = \\frac{Count\\space of\\space w\\space in\\space documents\\space of\\space class\\space C + 1}{Count\\space of\\space total\\space words\\space in\\space documents\\space of\\space class\\space C + |V| + 1}$$\n",
    "<br>\n",
    "Where we denote $|V|$ as the number of unique words in the vocabulary of our training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def count_vocabulary(sentences):\n",
    "    total_vocab = Counter()\n",
    "    for sentence in sentences:\n",
    "        total_vocab += Counter(sentence)\n",
    "    return total_vocab.keys(),len(total_vocab.keys())\n",
    "\n",
    "vocab,V = count_vocabulary(produce_sentences_from_dataframe(train).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.776\n"
     ]
    }
   ],
   "source": [
    "def conditional_prob_laplace_smoothing(word,label):\n",
    "    if word not in frequency_table:\n",
    "        frequency_table[word] = [0,0]\n",
    "    return (frequency_table[word][label] + 1) / (total_words[label] + V + 1)\n",
    "\n",
    "test_accuracy(probs_calc=conditional_prob_laplace_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Laplace smoothing made our classifier better by **8%** on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "In the previous implementation we used the rather straightforward Bag of Words approach (BOW) to compute the posterior probability of a sentence while relating to each word in a sentence as a feature.\n",
    "Another common approach that is used when analyzing sentences/documents is to use the tf-idf statistical measure.\n",
    "\n",
    "TF-stands for **term frequency**, this is usually the number of occurences of a word in our document (or sentence).\n",
    "IDF-stands for **inverse document frequency**, this is a measure of how much information the word provides us in our documents.<br>\n",
    "The most common way to compute the idf is by using the log scale of the inverse of the percentage of documents that contain our term, formally:<br>\n",
    "$$idf(t,D)=\\log \\frac{N} {\\mid\\{d\\in{D}:t\\in{d}\\}\\mid}$$\n",
    "Where:<br>\n",
    "**t**= The given term that we will compute idf for.<br>\n",
    "**D** = The group of all the documents in our dataset.<br>\n",
    "**N** = The total number of documents.<br>\n",
    "\n",
    "[Implementing TF-IDF for Naive Bayes](https://stackoverflow.com/questions/37405617/how-to-use-tf-idf-with-naive-bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we integrate TF-IDF into Naive Bayes\n",
    "\n",
    "What we did in the normal bag of words approach was to take the training data, compute the frequency of each word (which is a feature in our case) in each class of our data and use these frequencies in computing the probability of a sentence being classified to a certain class using bayes theorem. <br>\n",
    "<br>\n",
    "We can look at the counts of each word in all the documents of a given class as some sort of weight,all we need to do is switch the sum of counts with the sum of tfidf scores of each word, meaning that now the probability of a word w appearing in a document given that it's of class C is:\n",
    "$$Pr(w|c) = \\frac{sum\\space of\\space tfidf\\space scores\\space of\\space word\\space occurences\\space in\\space document\\space of\\space class\\space C + 1}{sum\\space of\\space tfidf\\space scores\\space of\\space all\\space the\\space words\\space in\\space all\\space the\\space documents\\space of\\space class\\space C + |V| + 1}$$\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.744\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def setUpWordIdf(data):\n",
    "    #This dictionary will hold for each word the number of sentences in the training data that contain it.\n",
    "    wordToIdf = {}\n",
    "    sentence_to_label = produce_sentences_from_dataframe(data)\n",
    "    for sentence,label in sentence_to_label.items():\n",
    "        for word in sentence:\n",
    "            if label == 1:\n",
    "                added_value = [0,1]\n",
    "            else:\n",
    "                added_value = [1,0]\n",
    "            if word in wordToIdf:\n",
    "                wordToIdf[word] = map(add,wordToIdf[word],added_value)\n",
    "            else:\n",
    "                wordToIdf[word] = added_value\n",
    "    return {key:[math.log(float(len(sentence_to_label.keys())) / (subval + 1e-6)) for subval in val] for key,val in wordToIdf.items()}\n",
    "\n",
    "wordToIdf = setUpWordIdf(train)\n",
    "wordToTF = copy.deepcopy(frequency_table)\n",
    "\n",
    "def preprocess_total_scores(sentences):\n",
    "    pos_tdidf_total = 0.0;neg_tfidf_total = 0.0\n",
    "    \n",
    "    for word in vocab:\n",
    "        neg_tfidf_total += (wordToTF[word][0] ** 2) * wordToIdf[word][0]\n",
    "        pos_tdidf_total += (wordToTF[word][1]** 2)  * wordToIdf[word][1]\n",
    "    \n",
    "    return [neg_tfidf_total,pos_tdidf_total]\n",
    "    \n",
    "tfidf_totals = preprocess_total_scores(produce_sentences_from_dataframe(train))\n",
    "\n",
    "def tfidf_probs(word,label):\n",
    "    if word not in wordToIdf:\n",
    "        wordToIdf[word] = [0.0,0.0]\n",
    "        wordToTF[word] = [0.0,0.0]\n",
    "    return float((wordToTF[word][label] * wordToIdf[word][label] + 1)) / (tfidf_totals[label] + V + 1)\n",
    "\n",
    "test_accuracy(probs_calc=tfidf_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
